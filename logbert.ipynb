{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6fba55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>machine</th>\n",
       "      <th>event</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>event_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[2180, 1497, 5653, 2180, 2284, 46, 5653, 4083,...</td>\n",
       "      <td>[15723664, 15778333, 16480956, 16496454, 16502...</td>\n",
       "      <td>1163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[534, 1063, 3188, 3387, 9654, 1462, 303, 537, ...</td>\n",
       "      <td>[16873871, 16874794, 16877089, 16878734, 16880...</td>\n",
       "      <td>1190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[347, 584, 63, 28, 63, 46, 993, 46, 28, 28, 28...</td>\n",
       "      <td>[16903523, 16905797, 16905877, 16906056, 16907...</td>\n",
       "      <td>2167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>[28, 913, 63, 28, 11186, 584, 28, 5680, 28, 46...</td>\n",
       "      <td>[16907322, 16910168, 16910331, 16910693, 16910...</td>\n",
       "      <td>1369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>[2559, 2559, 2559, 2559, 2722, 2559, 2559, 255...</td>\n",
       "      <td>[16874400, 16874400, 16875628, 16875628, 16876...</td>\n",
       "      <td>2222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12698</th>\n",
       "      <td>22448</td>\n",
       "      <td>[772, 772, 772, 772, 772]</td>\n",
       "      <td>[17065915, 17065915, 17077248, 17094401, 17094...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12699</th>\n",
       "      <td>22449</td>\n",
       "      <td>[4, 4, 4, 4, 4]</td>\n",
       "      <td>[16973806, 16973806, 16973806, 16973806, 16973...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12700</th>\n",
       "      <td>22450</td>\n",
       "      <td>[4, 4, 4, 4, 4]</td>\n",
       "      <td>[16971756, 16971756, 16971756, 16971756, 16971...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12705</th>\n",
       "      <td>22462</td>\n",
       "      <td>[4, 2, 2, 2, 4]</td>\n",
       "      <td>[16926797, 16928932, 16939825, 16943246, 16944...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30910</th>\n",
       "      <td>87748</td>\n",
       "      <td>[17230, 16597, 16597, 17230, 16597, 17230, 5, ...</td>\n",
       "      <td>[0, 149754, 165346, 170563, 253552, 623384, 62...</td>\n",
       "      <td>3021086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5393 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       machine                                              event  \\\n",
       "0            0  [2180, 1497, 5653, 2180, 2284, 46, 5653, 4083,...   \n",
       "1            1  [534, 1063, 3188, 3387, 9654, 1462, 303, 537, ...   \n",
       "2            2  [347, 584, 63, 28, 63, 46, 993, 46, 28, 28, 28...   \n",
       "3            5  [28, 913, 63, 28, 11186, 584, 28, 5680, 28, 46...   \n",
       "4            6  [2559, 2559, 2559, 2559, 2722, 2559, 2559, 255...   \n",
       "...        ...                                                ...   \n",
       "12698    22448                          [772, 772, 772, 772, 772]   \n",
       "12699    22449                                    [4, 4, 4, 4, 4]   \n",
       "12700    22450                                    [4, 4, 4, 4, 4]   \n",
       "12705    22462                                    [4, 2, 2, 2, 4]   \n",
       "30910    87748  [17230, 16597, 16597, 17230, 16597, 17230, 5, ...   \n",
       "\n",
       "                                               timestamp  event_count  \n",
       "0      [15723664, 15778333, 16480956, 16496454, 16502...         1163  \n",
       "1      [16873871, 16874794, 16877089, 16878734, 16880...         1190  \n",
       "2      [16903523, 16905797, 16905877, 16906056, 16907...         2167  \n",
       "3      [16907322, 16910168, 16910331, 16910693, 16910...         1369  \n",
       "4      [16874400, 16874400, 16875628, 16875628, 16876...         2222  \n",
       "...                                                  ...          ...  \n",
       "12698  [17065915, 17065915, 17077248, 17094401, 17094...            5  \n",
       "12699  [16973806, 16973806, 16973806, 16973806, 16973...            5  \n",
       "12700  [16971756, 16971756, 16971756, 16971756, 16971...            5  \n",
       "12705  [16926797, 16928932, 16939825, 16943246, 16944...            5  \n",
       "30910  [0, 149754, 165346, 170563, 253552, 623384, 62...      3021086  \n",
       "\n",
       "[5393 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "input_csv_path = \"microsoft/guide_alerts.csv\"\n",
    "df = pd.read_csv(input_csv_path)\n",
    "df = df.head(5_000_000)\n",
    "\n",
    "event_counts = df[\"event\"].value_counts()\n",
    "df[\"event_count\"] = df[\"event\"].map(event_counts)\n",
    "df[\"event\"] = df.apply(lambda row: 0 if row[\"event_count\"] < 3 else row[\"event\"], axis=1)\n",
    "df.drop(columns=[\"event_count\"], inplace=True)\n",
    "\n",
    "df_filtered = df[df['label'] == 0]\n",
    "df_sorted = df_filtered.sort_values(by=['machine', 'timestamp'])\n",
    "\n",
    "event_sequences = (\n",
    "    df_sorted\n",
    "    .groupby('machine')\n",
    "    .agg({\n",
    "        'event': list,\n",
    "        'timestamp': list   # keep timestamps as list\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "event_sequences['event_count'] = event_sequences['event'].apply(len)\n",
    "\n",
    "event_sequences = event_sequences[event_sequences['event_count'] >= 5]\n",
    "\n",
    "event_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4d4b87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['453 2 4 453 2', '4 4 2 2 2 4 2']\n",
      "Found 30905 unique events\n",
      "\n",
      "\n",
      "\n",
      "Vocabulary size: 15000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./event_tokenizer/tokenizer_config.json',\n",
       " './event_tokenizer/special_tokens_map.json',\n",
       " './event_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, processors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_event_tokenizer(df, vocab_size=10000):\n",
    "    \"\"\"\n",
    "    Create a tokenizer specifically for event sequences in log data\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(models.WordPiece())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    \n",
    "    # Special tokens for BERT-like models\n",
    "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "    event_counts = df['event'].value_counts()\n",
    "    print(f\"Found {len(event_counts)} unique events\")\n",
    "    top_events = event_counts.head(min(vocab_size - len(special_tokens) - 1000, len(event_counts))).index.tolist()\n",
    "    priority_tokens = [str(event) for event in top_events]\n",
    "    all_special_tokens = special_tokens + priority_tokens\n",
    "    \n",
    "    trainer = trainers.WordPieceTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=all_special_tokens,\n",
    "        min_frequency=1\n",
    "    )\n",
    "    \n",
    "    return tokenizer, trainer, priority_tokens\n",
    "\n",
    "# Use only benign sequences (label == 0) for training\n",
    "benign_sequences = event_sequences.copy()\n",
    "\n",
    "logs = []\n",
    "\n",
    "for _, row in benign_sequences.iterrows():\n",
    "    machine_id = row['machine']\n",
    "    event_list = row['event']\n",
    "    ts_list = row['timestamp']\n",
    "    \n",
    "    if len(event_list) >= 5:  # Only use sequences with at least 5 events\n",
    "        event_text = event_list\n",
    "        timestamp_text = ts_list\n",
    "        logs.append((machine_id, event_text, timestamp_text))\n",
    "\n",
    "train_logs, test_logs = train_test_split(logs, test_size=0.2, random_state=42)\n",
    "\n",
    "train_texts_sequence = [' '.join(map(str, event)) for _, event, _ in train_logs]\n",
    "print(train_texts_sequence[:2])\n",
    "\n",
    "tokenizer, trainer, priority_events = create_event_tokenizer(df, vocab_size=15000)\n",
    "\n",
    "tokenizer.train_from_iterator(train_texts_sequence, trainer)\n",
    "\n",
    "# BERT format\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "hf_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "hf_tokenizer.add_special_tokens({\n",
    "    'pad_token': '[PAD]',\n",
    "    'unk_token': '[UNK]',\n",
    "    'cls_token': '[CLS]',\n",
    "    'sep_token': '[SEP]',\n",
    "    'mask_token': '[MASK]'\n",
    "})\n",
    "\n",
    "print(f\"Vocabulary size: {hf_tokenizer.vocab_size}\")\n",
    "\n",
    "hf_tokenizer.save_pretrained('./event_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a21b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating windowed sequences with window_size=5, stride=3\n",
      "Created 1026803 windows from 5393 machines\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "class LogSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for creating windowed event sequences for BERT training\n",
    "    \"\"\"\n",
    "    def __init__(self, event_sequences, tokenizer, window_size=5, stride=3, mask_prob=0.15):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.mask_prob = mask_prob\n",
    "        self.windows = []\n",
    "        \n",
    "        print(f\"Creating windowed sequences with window_size={window_size}, stride={stride}\")\n",
    "        \n",
    "        for machine_id, events in event_sequences.items():\n",
    "            if len(events) >= window_size:\n",
    "                for i in range(0, len(events) - window_size + 1, stride):\n",
    "                    window = events[i:i + window_size]\n",
    "                    self.windows.append({\n",
    "                        'machine_id': machine_id,\n",
    "                        'events': window,\n",
    "                        'position': i\n",
    "                    })\n",
    "        \n",
    "        print(f\"Created {len(self.windows)} windows from {len(event_sequences)} machines\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        window = self.windows[idx]\n",
    "        events = window['events']\n",
    "        \n",
    "        event_text = ' '.join(map(str, events))\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            event_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.window_size + 2,  # +2 for [CLS] and [SEP]\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        labels = input_ids.clone()\n",
    "        special_tokens = {\n",
    "            self.tokenizer.cls_token_id,\n",
    "            self.tokenizer.sep_token_id,\n",
    "            self.tokenizer.pad_token_id\n",
    "        }\n",
    "        \n",
    "        maskable_positions = []\n",
    "        for i, token_id in enumerate(input_ids):\n",
    "            if token_id.item() not in special_tokens:\n",
    "                maskable_positions.append(i)\n",
    "        \n",
    "        num_mask = max(1, int(len(maskable_positions) * self.mask_prob))\n",
    "        mask_positions = random.sample(maskable_positions, min(num_mask, len(maskable_positions)))\n",
    "        \n",
    "        for pos in mask_positions:\n",
    "            rand = random.random()\n",
    "            if rand < 0.8:\n",
    "                input_ids[pos] = self.tokenizer.mask_token_id\n",
    "            elif rand < 0.9:\n",
    "                input_ids[pos] = random.randint(0, self.tokenizer.vocab_size - 1)\n",
    "        \n",
    "        # Set labels to -100 for non-masked positions (ignore in loss)\n",
    "        for i in range(len(labels)):\n",
    "            if i not in mask_positions:\n",
    "                labels[i] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "            'machine_id': window['machine_id']\n",
    "        }\n",
    "\n",
    "event_sequences_dict = {}\n",
    "for _, row in event_sequences.iterrows():\n",
    "    machine_id = row[\"machine\"]\n",
    "    events = row[\"event\"]\n",
    "    if len(events) >= 5:\n",
    "        event_sequences_dict[machine_id] = events\n",
    "\n",
    "window_size = 5\n",
    "stride = 3\n",
    "\n",
    "dataset = LogSequenceDataset(\n",
    "    event_sequences_dict,\n",
    "    hf_tokenizer,\n",
    "    window_size=window_size,\n",
    "    stride=stride,\n",
    "    mask_prob=0.15\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17e2a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertConfig, BertForMaskedLM, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "\n",
    "class LogBERT:\n",
    "    \"\"\"\n",
    "    BERT-based model for log anomaly detection using masked language modeling\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, tokenizer, device='cuda'):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "        # BERT configuration\n",
    "        self.config = BertConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=512,\n",
    "            num_hidden_layers=6,\n",
    "            num_attention_heads=8,\n",
    "            intermediate_size=2048,\n",
    "            max_position_embeddings=128,\n",
    "            type_vocab_size=2,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            sep_token_id=tokenizer.sep_token_id,\n",
    "            cls_token_id=tokenizer.cls_token_id,\n",
    "            mask_token_id=tokenizer.mask_token_id\n",
    "        )\n",
    "        \n",
    "        self.model = BertForMaskedLM(self.config)\n",
    "        self.model.to(device)\n",
    "            \n",
    "    def train_model(self, dataloader, epochs=3, learning_rate=5e-5, save_path='./logbert_model'):\n",
    "        \"\"\"\n",
    "        Train the LogBERT model using masked language modeling\n",
    "        \"\"\"\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        total_steps = len(dataloader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=int(0.1 * total_steps),\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "                \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "            \n",
    "            for batch_idx, batch in enumerate(progress_bar):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'avg_loss': f'{epoch_loss/(batch_idx+1):.4f}'\n",
    "                })\n",
    "\n",
    "                # if (batch_idx + 1) % 20000 == 0:\n",
    "                #     checkpoint_path = f\"{save_path}_checkpoint_{epoch}_{batch_idx}\"\n",
    "                #     os.makedirs(checkpoint_path, exist_ok=True)\n",
    "                #     self.model.save_pretrained(checkpoint_path)\n",
    "                #     self.tokenizer.save_pretrained(checkpoint_path)\n",
    "                #     print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "                            \n",
    "            avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "            print(f\"Epoch {epoch+1} completed. Average loss: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        self.model.save_pretrained(save_path)\n",
    "        self.tokenizer.save_pretrained(save_path)\n",
    "        print(f\"Final model saved to {save_path}\")\n",
    "\n",
    "        return total_loss / epochs * len(dataloader)\n",
    "\n",
    "    def calculate_sequence_probability(self, event_sequence, window_size=5):\n",
    "        \"\"\"\n",
    "        Calculate the probability of an event sequence using the trained model\n",
    "        This is used for anomaly detection - low probability indicates anomaly\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Convert sequence to string\n",
    "        if isinstance(event_sequence, list):\n",
    "            event_text = ' '.join(map(str, event_sequence))\n",
    "        else:\n",
    "            event_text = event_sequence\n",
    "        encoding = self.tokenizer(\n",
    "            event_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=window_size + 2,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            predictions = outputs.logits\n",
    "            probabilities = F.softmax(predictions, dim=-1)\n",
    "            \n",
    "\n",
    "            log_prob_sum = 0.0\n",
    "            token_count = 0\n",
    "            \n",
    "            for i in range(1, input_ids.size(1) - 1):\n",
    "                if attention_mask[0, i] == 1:\n",
    "                    token_id = input_ids[0, i].item()\n",
    "                    if token_id not in [self.tokenizer.pad_token_id, \n",
    "                                      self.tokenizer.cls_token_id, \n",
    "                                      self.tokenizer.sep_token_id]:\n",
    "                        token_prob = probabilities[0, i, token_id].item()\n",
    "                        log_prob_sum += math.log(token_prob + 1e-12)\n",
    "                        token_count += 1\n",
    "            \n",
    "            if token_count > 0:\n",
    "                avg_log_prob = log_prob_sum / token_count\n",
    "                return avg_log_prob\n",
    "            else:\n",
    "                return float('-inf')\n",
    "logbert = LogBERT(\n",
    "    vocab_size=hf_tokenizer.vocab_size,\n",
    "    tokenizer=hf_tokenizer,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efcdcdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16044/16044 [10:14<00:00, 26.13it/s, loss=1.1441, avg_loss=1.7788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Average loss: 1.7788\n",
      "Final model saved to ./logbert_model_alert\n",
      "Training completed! Average loss: 457884294.8290\n"
     ]
    }
   ],
   "source": [
    "avg_loss = logbert.train_model(\n",
    "    dataloader, \n",
    "    epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    save_path='./logbert_model_alert'\n",
    ")\n",
    "print(f\"Training completed! Average loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef771c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BERTCandidateClassifier:\n",
    "    def __init__(self, model, tokenizer, device='cuda', top_k=5):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.top_k = top_k\n",
    "        self.model.eval()\n",
    "        \n",
    "    def create_timestamp_windows(self, df, machine_id, target_event, target_timestamp, window_size=5):\n",
    "        machine_data = df[df['machine'] == machine_id].sort_values('timestamp')\n",
    "        target_row = machine_data[\n",
    "            (machine_data['timestamp'] == target_timestamp) & \n",
    "            (machine_data['event'] == target_event)\n",
    "        ]\n",
    "        if target_row.empty:\n",
    "            return []\n",
    "        \n",
    "        target_index = machine_data.index.get_loc(target_row.index[0])\n",
    "        \n",
    "        windows = []\n",
    "        events = machine_data['event'].tolist()\n",
    "        timestamps = machine_data['timestamp'].tolist()\n",
    "        labels = machine_data['label'].tolist()\n",
    "        \n",
    "        for start_offset in range(window_size - 1, -1, -1):\n",
    "            start_idx = max(0, target_index - start_offset)\n",
    "            end_idx = min(len(events), start_idx + window_size)\n",
    "            \n",
    "            if end_idx - start_idx == window_size and target_index >= start_idx and target_index < end_idx:\n",
    "                window_events = events[start_idx:end_idx]\n",
    "                window_timestamps = timestamps[start_idx:end_idx]\n",
    "                window_labels = labels[start_idx:end_idx]\n",
    "                target_position = target_index - start_idx\n",
    "                \n",
    "                windows.append({\n",
    "                    'events': window_events,\n",
    "                    'timestamps': window_timestamps,\n",
    "                    'labels': window_labels,\n",
    "                    'target_position': target_position,\n",
    "                    'start_idx': start_idx,\n",
    "                    'end_idx': end_idx - 1\n",
    "                })\n",
    "        \n",
    "        return windows\n",
    "    \n",
    "    def get_top_k_candidates(self, masked_sequence, mask_position):\n",
    "        \"\"\"\n",
    "        Use BERT classification head to get top K candidates for the masked position\n",
    "        Assumes only 1 masked position per inference\n",
    "        \"\"\"\n",
    "        event_text = ' '.join(map(str, masked_sequence))\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            event_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=10,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            predictions = outputs.logits\n",
    "            probabilities = F.softmax(predictions, dim=-1)\n",
    "            \n",
    "            # Find the position of [MASK] token in the tokenized sequence\n",
    "            mask_token_id = self.tokenizer.mask_token_id\n",
    "            mask_token_position = None\n",
    "            \n",
    "            for i, token_id in enumerate(input_ids[0]):\n",
    "                if token_id == mask_token_id:\n",
    "                    mask_token_position = i\n",
    "                    break\n",
    "            \n",
    "            if mask_token_position is None:\n",
    "                print(\"Warning: No [MASK] token found in sequence\")\n",
    "                return [], []\n",
    "            \n",
    "            top_k_probs, top_k_ids = torch.topk(probabilities[0, mask_token_position], self.top_k)\n",
    "            candidates, probs = [], []\n",
    "            \n",
    "            for prob, tid in zip(top_k_probs, top_k_ids):\n",
    "                token = self.tokenizer.convert_ids_to_tokens([tid.item()])[0]\n",
    "                if token.startswith('##'):\n",
    "                    token = token[2:]\n",
    "                try:\n",
    "                    candidate = int(token)\n",
    "                except ValueError:\n",
    "                    candidate = token\n",
    "                candidates.append(candidate)\n",
    "                probs.append(prob.item())\n",
    "\n",
    "            return candidates, probs\n",
    "                \n",
    "    def detect_malicious_with_candidates(self, df, target_event, machine_id, target_timestamp, \n",
    "                                       window_size=5, min_windows=1):\n",
    "        \"\"\"\n",
    "        Detect if target event is malicious using top-K candidate approach with majority voting\n",
    "        \n",
    "        Args:\n",
    "            df: Full dataset\n",
    "            target_event: Event to analyze for maliciousness\n",
    "            machine_id: Machine ID to analyze\n",
    "            target_timestamp: Specific timestamp of the event\n",
    "            window_size: Size of sliding windows (default 5)\n",
    "            min_windows: Minimum number of windows required for analysis\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with analysis results including majority vote decision\n",
    "        \"\"\"\n",
    "        windows = self.create_timestamp_windows(df, machine_id, target_event, target_timestamp, window_size)\n",
    "        \n",
    "        if len(windows) < min_windows:\n",
    "            print(f\"Insufficient windows ({len(windows)}) for analysis. Need at least {min_windows}.\")\n",
    "            return None\n",
    "                \n",
    "        window_results = []\n",
    "        malicious_votes = 0\n",
    "        benign_votes = 0\n",
    "        \n",
    "        for i, window in enumerate(windows):\n",
    "            masked_events = window['events'].copy()\n",
    "            original_event = masked_events[window['target_position']]\n",
    "            masked_events[window['target_position']] = '[MASK]'\n",
    "            \n",
    "            # Get top-K candidates\n",
    "            candidates, probabilities = self.get_top_k_candidates(masked_events, window['target_position'])\n",
    "            \n",
    "            is_in_candidates = original_event in candidates\n",
    "            candidate_rank = candidates.index(original_event) + 1 if is_in_candidates else None\n",
    "            candidate_probability = probabilities[candidates.index(original_event)] if is_in_candidates else 0.0\n",
    "            \n",
    "            # print(f\"  Original event in top-{self.top_k}? {'YES' if is_in_candidates else 'NO'}\")\n",
    "            # if is_in_candidates:\n",
    "            #     print(f\"  Rank: {candidate_rank}, Probability: {candidate_probability:.6f}\")\n",
    "            \n",
    "            # Vote: malicious if NOT in top-K candidates\n",
    "            # if candidate_probability < 0.3:\n",
    "            #     malicious_votes += 1\n",
    "            #     is_malicious_vote = True\n",
    "            #     print(f\"  Vote: MALICIOUS (not in top-{self.top_k} candidates)\")\n",
    "            # else:\n",
    "            #     benign_votes += 1\n",
    "            #     is_malicious_vote = False\n",
    "            #     print(f\"  Vote: BENIGN (in top-{self.top_k} candidates)\")\n",
    "            is_malicious_vote = not is_in_candidates\n",
    "            if is_malicious_vote:\n",
    "                malicious_votes += 1\n",
    "                # print(f\"  Vote: MALICIOUS (not in top-{self.top_k} candidates)\")\n",
    "            else:\n",
    "                benign_votes += 1\n",
    "                # print(f\"  Vote: BENIGN (in top-{self.top_k} candidates)\")\n",
    "            \n",
    "            window_results.append({\n",
    "                'window_index': i,\n",
    "                'window_events': window['events'],\n",
    "                'masked_events': masked_events,\n",
    "                'original_event': original_event,\n",
    "                'target_position': window['target_position'],\n",
    "                'candidates': candidates,\n",
    "                'probabilities': probabilities,\n",
    "                'is_in_candidates': is_in_candidates,\n",
    "                'candidate_rank': candidate_rank,\n",
    "                'candidate_probability': candidate_probability,\n",
    "                'malicious_vote': is_malicious_vote,\n",
    "                'timestamps': window['timestamps'],\n",
    "                'labels': window['labels']\n",
    "            })\n",
    "        \n",
    "        # Majority rule decision\n",
    "        total_votes = malicious_votes + benign_votes\n",
    "        majority_threshold = total_votes / 2\n",
    "\n",
    "        final_decision_malicious = 1 if malicious_votes > majority_threshold else 0\n",
    "        confidence = max(malicious_votes, benign_votes) / total_votes\n",
    "                \n",
    "        result = {\n",
    "            'machine_id': machine_id,\n",
    "            'target_event': target_event,\n",
    "            'target_timestamp': target_timestamp,\n",
    "            'window_size': window_size,\n",
    "            'top_k': self.top_k,\n",
    "            'num_windows': len(windows),\n",
    "            'malicious_votes': malicious_votes,\n",
    "            'benign_votes': benign_votes,\n",
    "            'total_votes': total_votes,\n",
    "            'final_decision_malicious': final_decision_malicious,\n",
    "            'confidence': confidence,\n",
    "            'window_results': window_results,\n",
    "            'approach': 'bert_candidate_classification'\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def batch_analyze_with_candidates(self, df, events_to_analyze):\n",
    "        \"\"\"\n",
    "        Analyze multiple events using the candidate approach\n",
    "        \n",
    "        Args:\n",
    "            events_to_analyze: List of tuples (event, machine_id, timestamp)\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        predictions = []\n",
    "        \n",
    "        for event_info in events_to_analyze:\n",
    "            \n",
    "            if len(event_info) != 3:\n",
    "                print(f\"Invalid event format: {event_info}. Expected (event, machine_id, timestamp)\")\n",
    "                continue\n",
    "                \n",
    "            event, machine_id, timestamp = event_info\n",
    "            result = self.detect_malicious_with_candidates(\n",
    "                df, event, machine_id, timestamp\n",
    "            )\n",
    "            \n",
    "            if result:\n",
    "                results.append(result)\n",
    "                predictions.append(result['final_decision_malicious'])\n",
    "            else:\n",
    "                predictions.append(None)\n",
    "\n",
    "        return results, predictions\n",
    "\n",
    "candidate_classifier = BERTCandidateClassifier(\n",
    "    model=logbert.model,\n",
    "    tokenizer=hf_tokenizer,\n",
    "    device=logbert.device,\n",
    "    top_k=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd96678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED BATCHED VERSION OF BERTCandidateClassifier\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class OptimizedBERTCandidateClassifier(BERTCandidateClassifier):\n",
    "    \"\"\"\n",
    "    Optimized version of BERTCandidateClassifier with batched processing\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_top_k_candidates_batch(self, masked_sequences_batch, max_length=10):\n",
    "        \"\"\"\n",
    "        Batch process multiple masked sequences to get top K candidates for all of them at once\n",
    "        \n",
    "        Args:\n",
    "            masked_sequences_batch: List of masked sequences (each is a list of events with '[MASK]')\n",
    "            max_length: Maximum sequence length for padding\n",
    "            \n",
    "        Returns:\n",
    "            candidates_batch: List of candidate lists for each sequence\n",
    "            probabilities_batch: List of probability lists for each sequence\n",
    "        \"\"\"\n",
    "        if not masked_sequences_batch:\n",
    "            return [], []\n",
    "        \n",
    "        event_texts = []\n",
    "        for masked_sequence in masked_sequences_batch:\n",
    "            event_text = ' '.join(map(str, masked_sequence))\n",
    "            event_texts.append(event_text)\n",
    "        \n",
    "        encodings = self.tokenizer(\n",
    "            event_texts,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encodings['input_ids'].to(self.device)\n",
    "        attention_mask = encodings['attention_mask'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            predictions = outputs.logits\n",
    "            probabilities = F.softmax(predictions, dim=-1)\n",
    "            \n",
    "            mask_token_id = self.tokenizer.mask_token_id\n",
    "            \n",
    "            candidates_batch = []\n",
    "            probabilities_batch = []\n",
    "            \n",
    "            for seq_idx in range(input_ids.size(0)):\n",
    "                mask_position = None\n",
    "                for token_idx, token_id in enumerate(input_ids[seq_idx]):\n",
    "                    if token_id == mask_token_id:\n",
    "                        mask_position = token_idx\n",
    "                        break\n",
    "                \n",
    "                if mask_position is None:\n",
    "                    candidates_batch.append([])\n",
    "                    probabilities_batch.append([])\n",
    "                    continue\n",
    "                \n",
    "                top_k_probs, top_k_ids = torch.topk(\n",
    "                    probabilities[seq_idx, mask_position], \n",
    "                    self.top_k\n",
    "                )\n",
    "                \n",
    "                candidates, probs = [], []\n",
    "                for prob, tid in zip(top_k_probs, top_k_ids):\n",
    "                    token = self.tokenizer.convert_ids_to_tokens([tid.item()])[0]\n",
    "                    if token.startswith('##'):\n",
    "                        token = token[2:]\n",
    "                    try:\n",
    "                        candidate = int(token)\n",
    "                    except ValueError:\n",
    "                        candidate = token\n",
    "                    candidates.append(candidate)\n",
    "                    probs.append(prob.item())\n",
    "                \n",
    "                candidates_batch.append(candidates)\n",
    "                probabilities_batch.append(probs)\n",
    "        \n",
    "        return candidates_batch, probabilities_batch\n",
    "    \n",
    "    def detect_malicious_with_candidates_batch(self, df, events_to_analyze, window_size=5, min_windows=1):\n",
    "        \"\"\"\n",
    "        Batch process multiple events for malicious detection using candidate approach\n",
    "        \n",
    "        Args:\n",
    "            df: Full dataset\n",
    "            events_to_analyze: List of tuples (event, machine_id, timestamp)\n",
    "            window_size: Size of sliding windows\n",
    "            min_windows: Minimum number of windows required for analysis\n",
    "            \n",
    "        Returns:\n",
    "            results: List of analysis results\n",
    "            predictions: List of final predictions\n",
    "        \"\"\"\n",
    "        \n",
    "        # Prepare all windows and masked sequences for batch processing\n",
    "        batch_data = []\n",
    "        event_indices = []\n",
    "        \n",
    "        for event_idx, event_info in enumerate(events_to_analyze):\n",
    "            event, machine_id, timestamp = event_info\n",
    "            windows = self.create_timestamp_windows(df, machine_id, event, timestamp, window_size)\n",
    "            \n",
    "            if len(windows) < min_windows:\n",
    "                print(f\"Event {event_idx}: Insufficient windows ({len(windows)}) for analysis\")\n",
    "                continue\n",
    "            for window_idx, window in enumerate(windows):\n",
    "                masked_events = window['events'].copy()\n",
    "                original_event = masked_events[window['target_position']]\n",
    "                masked_events[window['target_position']] = '[MASK]'\n",
    "                \n",
    "                batch_data.append({\n",
    "                    'event_idx': event_idx,\n",
    "                    'window_idx': window_idx,\n",
    "                    'masked_sequence': masked_events,\n",
    "                    'original_event': original_event,\n",
    "                    'window': window,\n",
    "                    'event_info': event_info\n",
    "                })\n",
    "                event_indices.append(event_idx)\n",
    "        \n",
    "        if not batch_data:\n",
    "            print(\"No valid data to process\")\n",
    "            return [], []\n",
    "        masked_sequences = [item['masked_sequence'] for item in batch_data]\n",
    "        candidates_batch, probabilities_batch = self.get_top_k_candidates_batch(\n",
    "            masked_sequences, max_length=window_size + 2\n",
    "        )\n",
    "        \n",
    "        event_results = {}\n",
    "        for item, candidates, probs in zip(batch_data, candidates_batch, probabilities_batch):\n",
    "            event_idx = item['event_idx']\n",
    "            if event_idx not in event_results:\n",
    "                event_results[event_idx] = {\n",
    "                    'event_info': item['event_info'],\n",
    "                    'windows': [],\n",
    "                    'window_results': []\n",
    "                }\n",
    "            original_event = item['original_event']\n",
    "            is_in_candidates = original_event in candidates\n",
    "            candidate_rank = candidates.index(original_event) + 1 if is_in_candidates else None\n",
    "            candidate_probability = probs[candidates.index(original_event)] if is_in_candidates else 0.0\n",
    "            \n",
    "            window_result = {\n",
    "                'window_index': item['window_idx'],\n",
    "                'window_events': item['window']['events'],\n",
    "                'masked_events': item['masked_sequence'],\n",
    "                'original_event': original_event,\n",
    "                'target_position': item['window']['target_position'],\n",
    "                'candidates': candidates,\n",
    "                'probabilities': probs,\n",
    "                'is_in_candidates': is_in_candidates,\n",
    "                'candidate_rank': candidate_rank,\n",
    "                'candidate_probability': candidate_probability,\n",
    "                'malicious_vote': not is_in_candidates,\n",
    "                'timestamps': item['window']['timestamps'],\n",
    "                'labels': item['window']['labels']\n",
    "            }\n",
    "            \n",
    "            event_results[event_idx]['windows'].append(item['window'])\n",
    "            event_results[event_idx]['window_results'].append(window_result)\n",
    "        \n",
    "        # Process final results for each event\n",
    "        results = []\n",
    "        predictions = []\n",
    "        \n",
    "        for event_idx in sorted(event_results.keys()):\n",
    "            event_data = event_results[event_idx]\n",
    "            event, machine_id, timestamp = event_data['event_info']\n",
    "            window_results = event_data['window_results']\n",
    "            \n",
    "            if not window_results:\n",
    "                predictions.append(None)\n",
    "                continue\n",
    "            \n",
    "            malicious_votes = sum(1 for wr in window_results if wr['malicious_vote'])\n",
    "            benign_votes = len(window_results) - malicious_votes\n",
    "            total_votes = len(window_results)\n",
    "            majority_threshold = total_votes / 2\n",
    "            \n",
    "            final_decision_malicious = 1 if malicious_votes > majority_threshold else 0\n",
    "            confidence = max(malicious_votes, benign_votes) / total_votes\n",
    "            \n",
    "            result = {\n",
    "                'machine_id': machine_id,\n",
    "                'target_event': event,\n",
    "                'target_timestamp': timestamp,\n",
    "                'window_size': window_size,\n",
    "                'top_k': self.top_k,\n",
    "                'num_windows': len(window_results),\n",
    "                'malicious_votes': malicious_votes,\n",
    "                'benign_votes': benign_votes,\n",
    "                'total_votes': total_votes,\n",
    "                'final_decision_malicious': final_decision_malicious,\n",
    "                'confidence': confidence,\n",
    "                'window_results': window_results,\n",
    "                'approach': 'bert_candidate_classification_batch'\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            predictions.append(final_decision_malicious)\n",
    "        \n",
    "        return results, predictions\n",
    "    \n",
    "    def batch_analyze_with_candidates_optimized(self, df, events_to_analyze, batch_size=256):\n",
    "        \"\"\"\n",
    "        Optimized batch analysis that processes events in smaller chunks for memory efficiency\n",
    "        \n",
    "        Args:\n",
    "            df: Full dataset\n",
    "            events_to_analyze: List of tuples (event, machine_id, timestamp)\n",
    "            batch_size: Number of events to process in each batch\n",
    "            \n",
    "        Returns:\n",
    "            results: List of analysis results\n",
    "            predictions: List of final predictions\n",
    "        \"\"\"\n",
    "        \n",
    "        all_results = []\n",
    "        all_predictions = []\n",
    "        \n",
    "        for i in range(0, len(events_to_analyze), batch_size):\n",
    "            batch_events = events_to_analyze[i:i + batch_size]\n",
    "            \n",
    "            batch_results, batch_predictions = self.detect_malicious_with_candidates_batch(\n",
    "                df, batch_events\n",
    "            )\n",
    "            \n",
    "            all_results.extend(batch_results)\n",
    "            all_predictions.extend(batch_predictions)\n",
    "            \n",
    "            while len(all_predictions) < i + len(batch_events):\n",
    "                all_predictions.append(None)\n",
    "                \n",
    "        return all_results, all_predictions\n",
    "\n",
    "# Initialize optimized classifier\n",
    "optimized_candidate_classifier = OptimizedBERTCandidateClassifier(\n",
    "    model=logbert.model,\n",
    "    tokenizer=hf_tokenizer,\n",
    "    device=logbert.device,\n",
    "    top_k=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1827c757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event 107: Insufficient windows (0) for analysis\n",
      "Event 120: Insufficient windows (0) for analysis\n",
      "Event 250: Insufficient windows (0) for analysis\n",
      "Event 490: Insufficient windows (0) for analysis\n"
     ]
    }
   ],
   "source": [
    "benign_test = []\n",
    "\n",
    "for machine, events, timestamp in test_logs:\n",
    "    for event, timestamp in zip(events, timestamp):\n",
    "        benign_test.append((event, machine, timestamp))\n",
    "\n",
    "\n",
    "malicious_logs = list(df[df['label'] == 1][['event', 'machine', 'timestamp']].itertuples(index=False, name=None))\n",
    "\n",
    "events_to_analyze = benign_test + malicious_logs\n",
    "\n",
    "truth_label = [0] * len(benign_test) + [1] * len(malicious_logs)\n",
    "\n",
    "\n",
    "import random\n",
    "paired = list(zip(events_to_analyze, truth_label))\n",
    "sampled = random.sample(paired, 500)\n",
    "sampled_events, sampled_labels = zip(*sampled)\n",
    "sampled_events = list(sampled_events)\n",
    "sampled_labels = list(sampled_labels)\n",
    "\n",
    "results, predictions = optimized_candidate_classifier.batch_analyze_with_candidates_optimized(df, sampled_events, batch_size=8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d91f0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "none_count = len(list(filter(lambda x: x is None, predictions)))\n",
    "print(none_count)\n",
    "predictions = [1 if x is None else x for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dac2f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1920\n",
      "Precision: 1.0000\n",
      "Recall: 0.1871\n",
      "F1 Score: 0.3153\n",
      "Confusion Matrix:\n",
      "[[  3   0]\n",
      " [404  93]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(sampled_labels, predictions)\n",
    "precision = precision_score(sampled_labels, predictions)\n",
    "recall = recall_score(sampled_labels, predictions)\n",
    "f1 = f1_score(sampled_labels, predictions)\n",
    "conf_matrix = confusion_matrix(sampled_labels, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0991b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
